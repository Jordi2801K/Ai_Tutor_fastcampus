from typing import List
import random
import io
import base64
import time
import pandas as pd
import streamlit as st
from audio_recorder_streamlit import audio_recorder
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.schema import StrOutputParser, AIMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
import os


# # ê¸°ë³¸ ì„¤ì •
# í™”ë©´ ë ˆì´ì•„ì›ƒ wideë¡œ ì„¤ì •
st.set_page_config(layout="wide")

# í˜„ì¬ í˜ì´ì§€ ì´ˆê¸°í™”
if "curr_page" not in st.session_state:
    st.session_state["curr_page"] = "home"
    st.session_state["curr_topic"] = "home"

# ì˜¤ë””ì˜¤ ìë™ í”Œë ˆì´ í•¨ìˆ˜ì—ì„œ ì´ì´ì „ ì˜¤ë””ì˜¤ ì œê±°ë¥¼ ìœ„í•œ ì‘ì—…
if "prev_audio_bytes" not in st.session_state:
    st.session_state.prev_audio_bytes = None

# session_state.exam_context ì€ ê° ì‹œí—˜ì— ëŒ€í•´ í•„ìš”í•œ ì •ë³´ë“¤ì„ ë‹´ì•„ë†“ì€ session_state. ê¸°ë³¸ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬
if "exam_context" not in st.session_state:
    st.session_state.exam_context = {}

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)


# # ì˜¤ë””ì˜¤ ìë™ ì¬ìƒ í•¨ìˆ˜
def autoplay_audio(file_path: str):
    with open(file_path, "rb") as f:
        data = f.read()
        b64 = base64.b64encode(data).decode()
        md = f"""
            <audio controls autoplay="true">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
            """
        st.markdown(
            md,
            unsafe_allow_html=True,
        )


# # ìœ ì € ìŒì„± ì¸ì‹í•˜ëŠ” í•¨ìˆ˜
def recognize_speech():
    user_input = ""
    # ì§ˆë¬¸ì— ë‹µí•˜ê¸°
    audio_bytes = audio_recorder("talk", pause_threshold=2.0,)
    if audio_bytes == st.session_state.prev_audio_bytes:
        audio_bytes = None
    st.session_state.prev_audio_bytes = audio_bytes

    try:
        if audio_bytes:
            with st.spinner("ìŒì„± ì¸ì‹ì¤‘..."):
                with open("./tmp_audio.wav", "wb") as f:
                    f.write(audio_bytes)

                with open("./tmp_audio.wav", "rb") as f: 
                    transcript = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        language="en"
                    )
                    user_input = transcript.text
    except Exception as e:
        print(e)
        pass
    return user_input


# speaking ìœ í˜•ì— ë§ì¶° ë§¤í•‘í•´ì£¼ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
speaking_topic_to_topic_info_map = {
    'speaking__listen_and_answer': {'display_name': 'ë“£ê³  ì§ˆë¬¸ì— ë‹µí•˜ê¸°', 'emoji': 'ğŸ’­'},
    'speaking__express_an_opinion': {'display_name': 'ì˜ê²¬ ë§í•˜ê¸°', 'emoji': 'ğŸ—£ï¸'},
    'speaking__debate': {'display_name': 'í† ë¡ í•˜ê¸°', 'emoji': 'ğŸ‘©â€'},
    'speaking__describe_img': {'display_name': 'ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸°', 'emoji': 'ğŸï¸'},
    'speaking__describe_charts': {'display_name': 'ë„í‘œ ë³´ê³  ì„¤ëª…í•˜ê¸°', 'emoji': 'ğŸ“Š'},
}

# wrting ìœ í˜•ì— ë§ì¶° ë§¤í•‘í•´ì£¼ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
writing_topic_to_topic_info_map = {
    'writing__dictation': {'display_name': 'ë°›ì•„ì“°ê¸° ì‹œí—˜', 'emoji': 'âœï¸'},
    'writing__responding_to_an_email': {'display_name': 'ì´ë©”ì¼ ë‹µì¥í•˜ê¸°', 'emoji': 'âœ‰ï¸'},
    'writing__summarization': {'display_name': 'ì œì‹œë¬¸ ë‚´ìš©ì„ ìš”ì•½í•˜ê¸°', 'emoji': 'âœï¸'},
    'writing__writing_opinion': {'display_name': 'ìì‹ ì˜ ì˜ê²¬ì“°ê¸°', 'emoji': 'ğŸ“'},
}


# session_stateì˜ curr_pageì™€ curr_topicì„ í˜„ì¬ topicìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜
def go_to_topic(topic):
    st.session_state["curr_page"] = topic
    st.session_state["curr_topic"] = topic

# session_stateì˜ curr_pageë¥¼ 'result'ë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜
def go_to_result():
    st.session_state["curr_page"] = "result"

# ë©”ì¸í™”ë©´ì— topicë“¤ì„ ë„ì›Œì£¼ëŠ” í•¨ìˆ˜
def display_topic(topic, topic_info, key):
    with st.container(border=True):
        st.write(f"{topic_info['emoji']} **{topic_info['display_name']}**")
        st.button("ì‹œì‘", key=f"start_{topic}_{key}", on_click=go_to_topic, kwargs=dict(topic=topic))



#### ë©”ì¸ í˜ì´ì§€ ####
# # home í˜ì´ì§€
con = st.container()
if st.session_state["curr_page"] == "home":
    with con:
        st.title("Speaking & Writing ì–´í•™ ì‹œí—˜")
        tab1, tab2 = st.tabs(["Speaking ì‹œí—˜", "Writing ì‹œí—˜"])

        with tab1:
            cols = st.columns(2)
            for i, (topic, topic_info) in enumerate(speaking_topic_to_topic_info_map.items()):
                with cols[i % 2]:  # This will alternate between the two columns
                    display_topic(topic, topic_info, i)
        
        with tab2:
            cols = st.columns(2)
            for i, (topic, topic_info) in enumerate(writing_topic_to_topic_info_map.items()):
                with cols[i % 2]:  # This will alternate between the two columns
                    display_topic(topic, topic_info, i)


# speaking_and_answer í˜ì´ì§€
elif st.session_state["curr_page"] == "speaking__listen_and_answer":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]  # í˜„ì¬ topicì— ë§ëŠ” ì •ë³´ë“¤ì„ ê°€ì ¸ì˜´
    st.title(topic_info['display_name'])

    # csvì— ì €ì¥ëœ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_listen_and_answer_data():
        df = pd.read_csv("./data/1_speaking__listen_and_answer/question_and_audio.csv")
        return df

    # ì§ˆë¬¸ ê°€ì ¸ì˜´
    df = load_listen_and_answer_data()

    # session_state.exam_contextì— 'question'ì´ ì—†ìœ¼ë©´
    if "question" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]     # ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤í•˜ê²Œ ê°€ì ¸ì˜´

        question = sample["question"]
        audio_file_path = sample["audio_file_path"]

        # ê°ì¢… í•„ìš”í•œ ì •ë³´ë“¤ì„ exam_contextì— ì¶”ê°€
        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["question"] = question
        st.session_state.exam_context["audio_file_path"] = audio_file_path


    if st.button("ì‹œí—˜ ì‹œì‘"):  # ì‹œí—˜ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    # session_stateì˜ exam_context.exam_start ì—ì„œ .get()ì„ ì‚¬ìš©í•˜ì—¬ True or Falseê°’ì„ ë°›ì•„ì˜¤ëŠ”ë°, ê¸°ë³¸ì€ Falseë¡œ ì„¤ì •ë˜ì–´ìˆìŒ
    if st.session_state.exam_context.get("exam_start", False):  # ê¸°ë³¸ì€ Falseì´ë‚˜, exam_context.exam_startê°€ Trueì´ë©´
        if st.session_state.exam_context["do_speech"]:  # exam_context.do_speechê°€ Trueì´ë©´
            autoplay_audio(st.session_state.exam_context["audio_file_path"])    # ì˜¤ë””ì˜¤ ìë™ ì¬ìƒ
            st.session_state.exam_context["do_speech"] = False  # exam_context.do_speechë¥¼ Falseë¡œ ì„¤ì •

        if not st.session_state.exam_context["do_speech"]:  # exam_context.do_speechê°€ Falseì´ë©´
            recognized_text = recognize_speech()    # ìœ ì €ì˜ ì‘ë‹µì„ ë°›ìŒ
            st.session_state.exam_context["user_answer"] = recognized_text  # ìœ ì €ì˜ ì‘ë‹µì„ session_stateì— ì¶”ê°€

        if st.session_state.exam_context.get("user_answer"):    # ìœ ì €ì˜ ì‘ë‹µì´ ìˆìœ¼ë©´
            
            # ì§ˆë¬¸ê³¼ ìœ ì €ì˜ ì‘ë‹µì„ í‘œì‹œ
            with st.container(border=True):
                answer_text = f"""
                - Question: {st.session_state.exam_context["question"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)
            

            # ìœ ì €ì˜ ì‘ë‹µì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  í‰ê°€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜
            def get_speaking__listen_and_answer_result(answer_text):
                model = ChatOpenAI(model="gpt-4o-mini")
                class Score(BaseModel):
                    reason: str = Field(description="Questionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ ì¶”ë¡ í•˜ë¼. í•œêµ­ì–´ë¡œ.")
                    score: int = Field(description="Questionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼")
                parser = JsonOutputParser(pydantic_object=Score)
                format_instruction = parser.get_format_instructions()

                human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                    "{input}\n---\nQuestionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ ì¶”ë¡ í•´ì„œ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì¤˜. ë‹µì€ í•œêµ­ì–´ë¡œ, ì¡´ëŒ“ë§ë¡œ ì‘ì„±í•´ì¤˜. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ì¤˜.  : {format_instruction}",
                    partial_variables={"format_instruction": format_instruction})

                prompt_template = ChatPromptTemplate.from_messages([human_msg_prompt_template],)
                
                chain = prompt_template | model | parser
                return chain.invoke({"input": answer_text})

            
            # í‰ê°€ ê²°ê³¼ì™€ ì ìˆ˜ë¥¼ ì¶œë ¥
            with st.container(border=True):
                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):
                    result = get_speaking__listen_and_answer_result(answer_text)

                f"""
                {result['reason']}

                #### ì ìˆ˜: {result['score']} / 10

                """


# # express_an_opinion í˜ì´ì§€
elif st.session_state["curr_page"] == "speaking__express_an_opinion":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]  # í˜„ì¬ topicì— ë§ëŠ” ì •ë³´ë“¤ì„ ê°€ì ¸ì˜´
    st.title(topic_info['display_name'])

    # csvì— ì €ì¥ëœ ì§ˆë¬¸ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    @st.cache_data
    def load_speaking__express_an_opinion_data():
        df = pd.read_csv("./data/2_speaking__express_an_opinion/question_and_audio.csv")
        return df

    # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    df = load_speaking__express_an_opinion_data()

    # session_state.exam_contextì— 'question'ì´ ì—†ìœ¼ë©´
    if "question" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]     # ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤í•˜ê²Œ ê°€ì ¸ì˜´

        question = sample["question"]
        audio_file_path = sample["audio_file_path"]

        # ê°ì¢… í•„ìš”í•œ ì •ë³´ë“¤ì„ exam_contextì— ì¶”ê°€
        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["question"] = question
        st.session_state.exam_context["audio_file_path"] = audio_file_path


    if st.button("ì‹œí—˜ ì‹œì‘"):  # ì‹œí—˜ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):  # exam_context.exam_startê°€ Trueì´ë©´
        if st.session_state.exam_context["do_speech"]:
            autoplay_audio(st.session_state.exam_context["audio_file_path"])
            st.session_state.exam_context["do_speech"] = False

        if not st.session_state.exam_context["do_speech"]:  # exam_context.do_speechê°€ Falseì´ë©´
            recognized_text = recognize_speech()    # ìœ ì €ì˜ ì‘ë‹µì„ ë°›ìŒ
            st.session_state.exam_context["user_answer"] = recognized_text  # ìœ ì €ì˜ ì‘ë‹µì„ session_stateì— ì¶”ê°€

        if st.session_state.exam_context.get("user_answer"):    # ìœ ì €ì˜ ì‘ë‹µì´ ìˆìœ¼ë©´

            # ì§ˆë¬¸ê³¼ ìœ ì €ì˜ ì‘ë‹µì„ í‘œì‹œ
            with st.container(border=True):
                answer_text = f"""
                - Question: {st.session_state.exam_context["question"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)
                
            with st.container(border=True):
                # ìœ ì €ì˜ ì‘ë‹µì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  í‰ê°€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜
                def get_speaking__express_opinion_result(answer_text):
                    model = ChatOpenAI(model="gpt-4o-mini")
                    class Score(BaseModel):
                        reason: str = Field(description="Questionì— ëŒ€í•´ ì˜ê²¬ì„ ë§í•˜ëŠ” ì‹œí—˜ì´ë‹¤. ì˜ê²¬ì„ ì ì ˆíˆ êµ¬ì¡°ì ìœ¼ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ ì¶”ë¡ í•˜ë¼. í•œêµ­ì–´ë¡œ.")
                        score: int = Field(description="Questionì— ëŒ€í•´ Your Answerê°€ ì¶©ë¶„íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜ê²¬ì„ í‘œí˜„í–ˆëŠ”ì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼.")
                    parser = JsonOutputParser(pydantic_object=Score)
                    format_instruction = parser.get_format_instructions()

                    human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                        "{input}\n---\nQuestionì— ëŒ€í•´ Your Answerê°€ ì¶©ë¶„íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜ê²¬ì„ í‘œí˜„í–ˆëŠ”ì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì¤˜. ë‹µì€ í•œêµ­ì–´ë¡œ, ì¡´ëŒ“ë§ë¡œ ì‘ì„±í•´ì¤˜. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ì¤˜.  : {format_instruction}",
                        partial_variables={"format_instruction": format_instruction})

                    prompt_template = ChatPromptTemplate.from_messages([human_msg_prompt_template],)
                    
                    chain = prompt_template | model | parser
                    return chain.invoke({"input": answer_text})

                # í‰ê°€ ê²°ê³¼ì™€ ì ìˆ˜ë¥¼ ì¶œë ¥
                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):
                    result = get_speaking__express_opinion_result(answer_text)

                f"""
                {result['reason']}

                #### ì ìˆ˜: {result['score']} / 10

                """


# # debate í˜ì´ì§€
elif st.session_state["curr_page"] == "speaking__debate":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    con1 = st.container()
    con2 = st.container()

    user_input = ""

    # session_state.exam_contextì— 'model'ì´ ì—†ìœ¼ë©´
    if "model" not in st.session_state.exam_context:
        st.session_state.exam_context["model"] = ChatOpenAI(model="gpt-4o-mini")

    # session_state.exam_contextì— 'messages'ê°€ ì—†ìœ¼ë©´
    if "messages" not in st.session_state.exam_context:
        system_prompt = """\
            - ë„ˆëŠ” AI ì‹œí—˜ ê°ë…ì´ë‹¤.
            - userì˜ ì˜ì–´ ì‹¤ë ¥ì„ ìœ„í•´ ì–´ë– í•œ ì£¼ì œì— ëŒ€í•´ ì„œë¡œ ì§ˆë¬¸ê³¼ ë‹µì„í•˜ë©° í† ë¡ í•œë‹¤."""

        # ëª¨ë¸ ì„¤ì •
        model = st.session_state.exam_context["model"]
        # ë…¼ë€ì´ ë ë§Œí•œ ì§ˆë¬¸(í† ë¡  ì£¼ì œ) ìƒì„±
        question = model.invoke("Create a controversial question for me.").content

        # session_state.exam_context.messagesì— System messageì™€ AI message ì¶”ê°€
        st.session_state.exam_context["messages"] = [SystemMessage(content=system_prompt), AIMessage(content=question),]

        # í† ë¡  ì£¼ì œ ìŒì„± ìƒì„±
        speech_file_path = "tmp_speak.mp3"
        response = client.audio.speech.create(
            model="tts-1",
            voice="echo", # alloy, echo, fable, onyx, nova, and shimmer
            input=question
        )
        response.stream_to_file(speech_file_path)
        autoplay_audio(speech_file_path)    # ìë™ì¬ìƒ

    # messagesë¥¼ í‘œì‹œí•˜ëŠ” Container
    with con1:
        for message in st.session_state.exam_context['messages']:
            # isinstance()ëŠ” ì–´ë– í•œ ë³€ìˆ˜ê°€ í•´ë‹¹ íƒ€ì…ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
            if isinstance(message, SystemMessage):  # messageê°€ System messageì´ë©´
                continue    # ê±´ë„ˆëœ€ (í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            role = 'user' if message.type == 'human' else 'assistant'   # messageì˜ íƒ€ì…ì´ humanì´ë©´ roleì„ userë¡œ, ì•„ë‹ˆë©´ assatantë¡œ ì„¤ì •
            with st.chat_message(role):     # roleì— ë”°ë¼ ì•„ì´ì½˜ì„ ë‹¤ë¥´ê²Œí•˜ì—¬
                st.markdown(message.content)    # messageë¥¼ ì¶œë ¥

    # # ìŒì„± ì¸ì‹ì„ ìœ„í•œ Container
    # with con2:
    #     user_input = recognize_speech()
    

    # ìŒì„± ì¸ì‹ì„ ìœ„í•œ Container
    with con2:
        # í™”ë©´ í¬ê¸°ì— ë”°ë¼ ì˜¤ë””ì˜¤ ë…¹ìŒê¸° ë²„íŠ¼ì„ ì¤‘ì•™ì— ë°°ì¹˜
        col1, col2, col3 = st.columns([1, 2, 1])
        with col1:
            st.write("")  # ì™¼ìª½ ê³µë°±
        with col2:
            user_input = recognize_speech()
        with col3:
            st.write("")  # ì˜¤ë¥¸ìª½ ê³µë°±

    # ë‹¤ì‹œ messagesì˜ Containerë¡œ ì™€ì„œ
    with con1:
    
        # ëŒ€í™”ì˜ í„´ì´ 4ë²ˆ ë˜ë©´ ì¢…ë£Œì‹œí‚¤ê¸° ìœ„í•´ session_state.exam_context.messagesì˜ ê¸¸ì´ë¥¼ ì¸¡ì •
        turn_len = len(st.session_state.exam_context['messages'])
        max_turn_len = 5    # ì´ ëŒ€í™”ì˜ í„´ìˆ˜ 4ë²ˆì´ì§€ë§Œ session_state.exam_context.messages ì•ˆì— System messageê°€ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ 5ë¡œ ì„¤ì •

        if user_input and turn_len < max_turn_len:  # ìœ ì €ì˜ ì…ë ¥ì´ ì¡´ì¬í•˜ê³  ëŒ€í™”ì˜ í„´ì´ 5ë²ˆë³´ë‹¤ ì‘ìœ¼ë©´ ê³„ì† ì…ë ¥ì„ ë°›ê³  ëŒ€ë‹µì„ í•¨
            st.session_state.exam_context['messages'].append(HumanMessage(content=user_input))  # ìœ ì €ì˜ ì…ë ¥ì„ session_stateì— ì¶”ê°€

            # userì˜ messageë¥¼ ì¶œë ¥
            with st.chat_message("user"):
                st.markdown(user_input)
            
            # assistantì˜ messageë¥¼ streamì˜ í˜•íƒœë¡œ ì¶œë ¥
            with st.chat_message("assistant"):
                message_placeholder = st.empty()    # stream í•˜ê¸° ìœ„í•œ placeholder
                full_response = ""

                model = st.session_state.exam_context["model"]  # ëª¨ë¸ ì„¤ì •

                # session_state.exam_context.messagesë¥¼ ìˆœíšŒí•˜ë©´ì„œ message ì¶œë ¥
                for chunk in model.stream(st.session_state.exam_context['messages']):
                    full_response += (chunk.content or "")
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)

                # ìŒì„± ìƒì„±
                speech_file_path = "tmp_speak.mp3"
                response = client.audio.speech.create(
                model="tts-1",
                voice="echo", # alloy, echo, fable, onyx, nova, and shimmer
                input=full_response
                )
                response.stream_to_file(speech_file_path)

                autoplay_audio(speech_file_path)    # ìë™ ì¬ìƒ

            # session_stateì— AI message ì¶”ê°€
            st.session_state.exam_context['messages'].append(AIMessage(content=full_response))


        if turn_len >= max_turn_len:    # ëŒ€í™”ì˜ í„´ì´ 5ë²ˆ(4ë²ˆ ëŒ€í™” ìì²´ëŠ” 4ë²ˆ) ì´ìƒ ë˜ì—ˆìœ¼ë©´ ì…ë ¥ì„ ê·¸ë§Œ ë°›ê³  í† ë¡ ì— ëŒ€í•œ í‰ê°€ë¥¼ ì§„í–‰
            # ìœ ì €ì˜ ì‘ë‹µì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  í‰ê°€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜
            def get_speaking__debate_result(conversation):
                model = ChatOpenAI(model="gpt-4o-mini")
                class Score(BaseModel):
                    reason: str = Field(description="ì£¼ì–´ì§„ ëŒ€í™”ì— ëŒ€í•´ Userê°€ ì–¼ë§ˆë‚˜ ë…¼ë¦¬ì ì´ê³  ìœ ì°½í•˜ê²Œ ì˜ì–´ë¡œ ì‘ë‹µí•˜ì˜€ëŠ”ì§€ ì¶”ë¡ í•˜ë¼. í•œêµ­ì–´ë¡œ.")
                    score: int = Field(description="ì£¼ì–´ì§„ ëŒ€í™”ì—ì„œ Userì˜ ì‘ë‹µì— ëŒ€í•´ ìœ ì°½ì„±ê³¼ ë…¼ë¦¬ì„±ì„ ê³ ë ¤í•˜ì—¬ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼.")
                parser = JsonOutputParser(pydantic_object=Score)
                format_instruction = parser.get_format_instructions()

                human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                    "{input}\n---\nì£¼ì–´ì§„ ëŒ€í™”ì—ì„œ Userì˜ ì‘ë‹µì— ëŒ€í•´ ìœ ì°½ì„±ê³¼ ë…¼ë¦¬ì„±ì„ ê³ ë ¤í•˜ì—¬ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì¤˜. ë‹µì€ í•œêµ­ì–´ë¡œ, ì¡´ëŒ“ë§ë¡œ ì‘ì„±í•´ì¤˜. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ì¤˜.  : {format_instruction}",
                    partial_variables={"format_instruction": format_instruction})

                prompt_template = ChatPromptTemplate.from_messages([human_msg_prompt_template],)
                
                chain = prompt_template | model | parser
                return chain.invoke({"input": conversation})

                
            with st.container(border=True):
                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):

                    conversation = ""
                    # session_state.exam_context.messagesì— roleì„ ë¶€ì—¬í•˜ê³  ì±„ì ì„ ì§„í–‰
                    for msg in st.session_state.exam_context["messages"]:
                        role = 'User' if msg.type == 'human' else 'AI'
                        conversation += f"{role}: {msg.content}"

                    result = get_speaking__debate_result(conversation)

                # ë“±ê¸‰ ë¶€ì—¬
                grade = ""
                if result['score'] >= 8:
                    grade = "Advanced"
                elif 4 < result['score'] < 8:
                    grade = "Intermediate"
                elif result['score'] <= 4:
                    grade = "Novice"

                grade = f"{grade}, {result['score']} / 10"

                f"""
                {result['reason']}

                #### ë“±ê¸‰, ì ìˆ˜: {grade}
                """


# # describe_image í˜ì´ì§€
elif  st.session_state["curr_page"] == "speaking__describe_img":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # csvì— ì €ì¥ëœ ì´ë¯¸ì§€ì™€ ì„¤ëª… ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    @st.cache_data
    def load_speaking__describe_img():
        df = pd.read_csv("./data/3_speaking__describe_img/desc_img.csv")
        return df

    # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    df = load_speaking__describe_img()

    # session_state.exam_contextì— 'img_path'ê°€ ì—†ìœ¼ë©´
    if "img_path" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]     # ëœë¤í•˜ê²Œ ì´ë¯¸ì§€ í•˜ë‚˜ ê°€ì ¸ì˜´

        img_path = sample["img_path"]   # ì´ë¯¸ì§€ ê²½ë¡œ ë³€ìˆ˜ë¡œ ì €ì¥
        desc = sample["desc"]   # ì„¤ëª… ë³€ìˆ˜ë¡œ ì €ì¥

        # session_state.exam_contextì— ê° ì •ë³´ë“¤ ì €ì¥
        st.session_state.exam_context["img_path"] = img_path
        st.session_state.exam_context["desc"] = desc
        st.session_state.exam_context["recognized_text"] = ""

    
    # í™”ë©´ì— ì´ë¯¸ì§€ í‘œì‹œ
    st.image(st.session_state.exam_context['img_path'])
    
    # ìœ ì €ì—ê²Œ ì´ë¯¸ì§€ì— ëŒ€í•œ ë¬˜ì‚¬(ì„¤ëª…)ì„ ì‘ë‹µë°›ìŒ
    with st.container(border=True):
        recognized_text = recognize_speech()
        if recognized_text:     # ìŒë‹µì´ ìˆìœ¼ë©´
            st.session_state.exam_context["recognized_text"] = recognized_text  # session_state.exam_context.recognized_textì— ì¶”ê°€
        st.write(st.session_state.exam_context["recognized_text"])  # ì‘ë‹µì„ í™”ë©´ì— í‘œì‹œ

    if st.button("ì œì¶œí•˜ê¸°"):   # ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
        # ìœ ì €ì˜ ì‘ë‹µê³¼ ì •ë‹µì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
        def get_speaking__describe_img(user_input, ref):
            model = ChatOpenAI(model="gpt-4o-mini", temperature=0.8) # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨
            
            class Evaluation(BaseModel):
                score: int = Field(description="ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸° í‘œí˜„ í‘œí˜„ ì ìˆ˜. 0~10ì ")
                feedback: str = Field(description="ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸°ë¥¼ ë” ì˜ í•  ìˆ˜ ìˆë„ë¡í•˜ëŠ” ìì„¸í•œ í”¼ë“œë°±. Markdowní˜•ì‹, í•œêµ­ì–´ë¡œ.")
            
            parser = JsonOutputParser(pydantic_object=Evaluation)
            format_instructions = parser.get_format_instructions()

            human_prompt_template = HumanMessagePromptTemplate.from_template(
                            "ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸° ì˜ì–´ ì‹œí—˜ì´ì•¼. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ ë¬˜ì‚¬ì—ì„œ ë¶€ì¡±í•œ ì ì´ ìˆëŠ”ì§€ í‰ê°€í•´ì¤˜. ë¶€ì¡±í•œ ì ì´ ë§ê±°ë‚˜ ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë‹¨ìˆœí•˜ë©´ í™•ì‹¤í•˜ê²Œ ë‚®ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  í‰ê°€ë„ ê·¸ì— ë§ê²Œ í•´ì¤˜. ë¶€ì¡±í•œ ì ì´ ìˆë‹¤ë©´ ì–´ë–¤ì ì´ ë¶€ì¡±í•œì§€, ì„¤ëª…ì— ì¶”ê°€ë˜ë©´ ì¢‹ì„ë§Œí•œ ê²ƒë“¤ì„ ì„¤ëª…í•´ì¤˜. í‰ê°€ì™€ ì„¤ëª…ì€ ì¡´ëŒ“ë§, í•œêµ­ì–´ë¡œ í•´ì¤˜.\nì‚¬ìš©ìì˜ ì‘ë‹µ: {input}\Reference: {ref}\në‹¤ìŒ í¬ë©§ì— ë§ì¶°ì„œ ì‘ë‹µí•´ì¤˜ : {format_instructions}",
                                        partial_variables={"format_instructions": format_instructions})

            prompt = ChatPromptTemplate.from_messages([human_prompt_template,])
            eval_chain = prompt | model | parser

            result = eval_chain.invoke({"input": user_input, "ref": ref})
            return result


        st.title("ê²°ê³¼ & í”¼ë“œë°±- ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸°")

        with st.spinner("ê²°ê³¼ & í”¼ë“œë°± ìƒì„±ì¤‘..."):
            # ìœ ì €ì˜ ì‘ë‹µê³¼ ì •ë‹µì„ ë¹„êµí•˜ì—¬ í‰ê°€ ì§„í–‰
            result = get_speaking__describe_img(user_input=recognized_text,
                                                ref=st.session_state.exam_context['desc'])

            grade = ""
            if result['score'] >= 8:
                grade = "ê³ ê¸‰"
            elif 4 < result['score'] < 8:
                grade = "ì¤‘ê¸‰"
            elif result['score'] <= 4:
                grade = "ì´ˆê¸‰"

            grade = f"{grade} ({result['score']} / 10)"

            f"""
            ë‹¹ì‹ ì´ ì œê³µí•œ ë‹µë³€ì€ ìŠ¤í”¼í‚¹ ì‚¬ì§„ ë¬˜ì‚¬ ì‹œí—˜ì—ì„œ `{grade}` ìˆ˜ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ê¸° ì¢‹ì€ ì ‘ê·¼ì…ë‹ˆë‹¤.
            
            ì—¬ê¸° ëª‡ê°€ì§€ í”¼ë“œë°±ì„ ë“œë¦½ë‹ˆë‹¤.

            {result['feedback']}
            """


# # describe_charts í˜ì´ì§€
elif  st.session_state["curr_page"] == "speaking__describe_charts":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # csvì— ì €ì¥ëœ ë„í‘œì™€ ì„¤ëª… ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    @st.cache_data
    def load_speaking__describe_charts():
        df = pd.read_csv("./data/4_speaking__describe_charts/desc_charts.csv")
        return df

    # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    df = load_speaking__describe_charts()

    # session_state.exam_contextì— 'img_path'ê°€ ì—†ìœ¼ë©´
    if "img_path" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]     # ë„í‘œ ëœë¤í•˜ê²Œ í•˜ë‚˜ ê°€ì ¸ì˜´

        img_path = sample["img_path"]   # ë„í‘œ ê²½ë¡œ
        desc = sample["desc"]   # ë„í‘œì— ëŒ€í•œ ì„¤ëª…

        # session_state.exam_contextì— ê° ì •ë³´ë“¤ ì €ì¥
        st.session_state.exam_context["img_path"] = img_path
        st.session_state.exam_context["desc"] = desc
        st.session_state.exam_context["recognized_text"] = ""

    # í™”ë©´ì— ë„í‘œ ì¶œë ¥
    st.image(st.session_state.exam_context['img_path'])
    
    # ìœ ì €ì—ê²Œ ë„í‘œì— ëŒ€í•œ ì„¤ëª…ì„ ì‘ë‹µë°›ìŒ
    with st.container(border=True):
        recognized_text = recognize_speech()
        if recognized_text:     # ì‘ë‹µì´ ìˆìœ¼ë©´
            st.session_state.exam_context["recognized_text"] = recognized_text  # session_state.exam_context.recognized_textì— ì¶”ê°€
        st.write(st.session_state.exam_context["recognized_text"])  # ì‘ë‹µì„ í•˜ë©´ì— í‘œì‹œ

    if st.button("ì œì¶œí•˜ê¸°"):   # ë²„íŠ¼ ëˆ„ë¥´ë©´
        # ìœ ì €ì˜ ì‘ë‹µê³¼ ì •ë‹µì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
        def get_speaking__describe_img(user_input, ref):
            model = ChatOpenAI(model="gpt-4o-mini", temperature=0.8) # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨
            class Evaluation(BaseModel):
                score: int = Field(description="ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸° ì ìˆ˜. 0~10ì ")
                feedback: str = Field(description="ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸° ì ìˆ˜. Markdowní˜•ì‹, í•œêµ­ì–´ë¡œ.")
            parser = JsonOutputParser(pydantic_object=Evaluation)
            format_instructions = parser.get_format_instructions()

            human_prompt_template = HumanMessagePromptTemplate.from_template(
                            "ë„í‘œë³´ê³  ì„¤ëª…í•˜ê¸° ì˜ì–´ ì‹œí—˜ì´ì•¼. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ ì˜¬ë°”ë¥´ê²Œ ë„í‘œë¥¼ ì„¤ëª…í–ˆëŠ”ì§€ í‰ê°€í•´ì¤˜. ì •í™•í•˜ì§€ ì•Šê±°ë‚˜ ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë‹¨ìˆœí•˜ë©´ í™•ì‹¤í•˜ê²Œ ë‚®ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  í‰ê°€ë„ ê·¸ì— ë§ê²Œ í•´ì¤˜. ì •í™•í•˜ì§€ ì•Šë‹¤ë©´ ì–´ë–¤ì ì´ ì •í™•í•˜ì§€ ì•Šì€ì§€, ì¶”ê°€ë¡œ ì„¤ëª…í•˜ë©´ ì¢‹ì„ë§Œí•œ ê²ƒë“¤ì„ ì„¤ëª…í•´ì¤˜. í‰ê°€ì™€ ì„¤ëª…ì€ ì¡´ëŒ“ë§›, í•œêµ­ì–´ë¡œ í•´ì¤˜.\nì‚¬ìš©ìì˜ ì‘ë‹µ: {input}\Reference: {ref}\në‹¤ìŒ í¬ë§·ì— ë§ì¶°ì„œ ì‘ë‹µí•´ì¤˜ : {format_instructions}",
                                        partial_variables={"format_instructions": format_instructions})

            prompt = ChatPromptTemplate.from_messages([human_prompt_template,])
            eval_chain = prompt | model | parser

            result = eval_chain.invoke({"input": user_input, "ref": ref})
            return result


        st.title("ê²°ê³¼ & í”¼ë“œë°±- ë„í‘œ ë³´ê³  ì„¤ëª…í•˜ê¸°")

        with st.spinner("ê²°ê³¼ & í”¼ë“œë°± ìƒì„±ì¤‘..."):

            result = get_speaking__describe_img(user_input=recognized_text,
                                                ref=st.session_state.exam_context['desc'])
        
            grade = ""
            if result['score'] >= 8:
                grade = "ê³ ê¸‰"
            elif 4 < result['score'] < 8:
                grade = "ì¤‘ê¸‰"
            elif result['score'] <= 4:
                grade = "ì´ˆê¸‰"

            grade = f"{grade} ({result['score']} / 10)"

            f"""
            ë‹¹ì‹ ì´ ì œê³µí•œ ë‹µë³€ì€ ìŠ¤í”¼í‚¹ ì‚¬ì§„ ë¬˜ì‚¬ ì‹œí—˜ì—ì„œ `{grade}` ìˆ˜ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ê¸° ì¢‹ì€ ì ‘ê·¼ì…ë‹ˆë‹¤.
            
            ì—¬ê¸° ëª‡ê°€ì§€ í”¼ë“œë°±ì„ ë“œë¦½ë‹ˆë‹¤.

            {result['feedback']}
            """


# # writing_dictation í˜ì´ì§€
elif  st.session_state["curr_page"] == "writing__dictation":
    topic_info = writing_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # csvì— ì €ì¥ëœ ë¬¸ì¥ê³¼ ìŒì„± ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    @st.cache_data
    def load_writing__dictation():
        df = pd.read_csv("./data/5_writing__dictation/sent_and_audio.csv")
        return df

    # ë¬¸ì¥, ìŒì„± ê°€ì ¸ì˜¤ê¸°
    df = load_writing__dictation()

    # session_state.exam_contextì— 'sentence'ê°€ ì—†ìœ¼ë©´
    if "sentence" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]     # ë¬¸ì¥ê³¼ ìŒì„±ì„ ëœë¤í•˜ê²Œ ê°€ì ¸ì˜¤ê³ 

        sentence = sample["sentence"]
        audio_file_path = sample["audio_file_path"]

        # session_state.exam_contextì— ê° ì •ë³´ë“¤ ì €ì¥
        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["sentence"] = sentence
        st.session_state.exam_context["audio_file_path"] = audio_file_path


    if st.button("ì‹œí—˜ ì‹œì‘"):  # ë²„íŠ¼ ëˆ„ë¥´ë©´
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):  # ê¸°ë³¸ì€ Falseì´ì§€ë§Œ, exam_startê°€ Trueì´ë©´
        if st.session_state.exam_context["do_speech"]:  # do_speechê°€ Trueì´ë©´
            autoplay_audio(st.session_state.exam_context["audio_file_path"])    # ë¬¸ì¥ì˜ ìŒì„± ì¬ìƒ
            st.session_state.exam_context["do_speech"] = False  # do_speechë¥¼ Falseë¡œ ì„¤ì •


        # ìœ ì €ì˜ ë°›ì•„ì“°ê¸° ì…ë ¥ì„ ë°›ìŒ
        user_answer = st.text_input("user answer")
        if user_answer: # ìœ ì €ì˜ ì…ë ¥ì´ ìˆìœ¼ë©´
            st.session_state.exam_context["user_answer"] = user_answer  # session_state.exam_context.user_answerì— ì¶”ê°€

        # session_state.exam_context.user_answerê°€ ìˆìœ¼ë©´ == ìœ ì €ì˜ ë°›ì•„ì“°ê¸° ì…ë ¥ì´ ìˆìœ¼ë©´
        if st.session_state.exam_context.get("user_answer"):
            # ë°›ì•„ì“°ê¸°ì˜ ì •ë‹µê³¼ ìœ ì €ì˜ ì…ë ¥ì„ ì¶œë ¥
            with st.container(border=True):
                answer_text = f"""
                - Original sentence: {st.session_state.exam_context["sentence"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """
                st.markdown(answer_text)
                
            # ìœ ì €ì˜ ì…ë ¥ê³¼ ì •ë‹µì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
            def get_writing__dictation_result(answer_text, ref):
                model = ChatOpenAI(model="gpt-4o-mini")
                class Evaluation(BaseModel):
                    reason: str = Field(description="ë°›ì•„ì“°ê¸° í‰ê°€ë¥¼ ìœ„í•œ ì¶”ë¡ ")
                    score: int = Field(description="ë°›ì•„ì“°ê¸° ì ìˆ˜. 0~10ì ")
                parser = JsonOutputParser(pydantic_object=Evaluation)
                format_instruction = parser.get_format_instructions()

                human_prompt_template = HumanMessagePromptTemplate.from_template(
                                            "ì˜ì–´ ë°›ì•„ì“°ê¸° ì‹œí—˜ì´ì•¼. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ ë¶„ì„í•˜ê³  í‰ê°€í•´ì¤˜. ì„¤ëª…ì€ ì¡´ëŒ“ë§, í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì¤˜.\nì‚¬ìš©ìì˜ ì‘ë‹µ: {input}\Reference: {ref}\në‹¤ìŒ í¬ë§·ì— ë§ì¶°ì„œ ì‘ë‹µí•´ì¤˜ : {format_instructions}",
                                            partial_variables={"format_instructions": format_instruction})

                prompt_template = ChatPromptTemplate.from_messages([human_prompt_template,])

                chain = prompt_template | model | parser
                return chain.invoke({"input": answer_text, "ref": ref})
                
            with st.container(border=True):
                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):
                    # AIê°€ ì±„ì í•œ ê²°ê³¼
                    model_result = get_writing__dictation_result(answer_text, st.session_state.exam_context['sentence'])
                    model_score = model_result['score']

                f"""
                ### í‰ê°€ ê²°ê³¼
                {model_result['reason']}
                ì ìˆ˜: {model_score} / 10
                """